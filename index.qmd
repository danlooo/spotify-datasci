---
title: "Predicting music genre of spotify tracks using deep learning"
author: "Daniel Loos"
date: today
editor: visual
format:
  html:
    toc: true
execute:
  cache: true
---

```{r setup, echo=FALSE}

knitr::opts_knit$set(root.dir = here::here())

options(warn=-1)
```

[This project on GitHub](https://github.com/danlooo/spotify-datasci)

## Abstract

Music genres feature particular patterns that can be used to predict the genre of a track by analyzing its pitch sequences. The Spotify API provides features for entire tracks, e.g. its loudness or acousticness scores, as well as the sequence of the pitches. About half the the variance of totaling 3600 tracks across techno, rock, jazz and classicsal generes could be explained by these summary features alone. However, there was no strong separation between the genre clusters. Deep learning was then applied to find patterns of the detailed pitch sequence. However, only 30% of the test tracks could be classified correctly using CNN or LSTM architectures.

## ETL pipeline

```{r}
source("_targets.R")
tar_load(c("terms", "track_audio_features", "selected_audio_features", "audio_analyses", "track_train_test_split", "track_searches", "track_pitches", "valid_tracks", "track_audio_analyses"))

tar_visnetwork()
```

## Data overview

Spotify was queried by the following terms. Up to 50 tracks per term were retrieved.

```{r}
terms
```

Total number of tracks:

```{r}
nrow(track_searches)
```

Tracks per term:

```{r}
track_searches |> count(term)
```

Features per track:

```{r}
tracks <-
  track_audio_features |>
  left_join(track_searches, by = "id") |>
  filter(id %in% valid_tracks) |>
  mutate(term = factor(term))

colnames(tracks)
```

Number of tracks after sanity checks:

```{r}
nrow(tracks)
```

## Track features per term

```{r}
features <- c("danceability", "acousticness")

tar_load(track_train_test_split)
tracks_train <- tracks |> inner_join(track_train_test_split) |> filter(is_train)

tracks_train |>
  select(term, features) |>
  mutate(across(features, scale)) |>
  pivot_longer(features) |>
  ggplot(aes(term, value)) +
    geom_quasirandom() +
    geom_boxplot(outlier.size = NULL, width = 0.5) +
    facet_wrap(~ name, scales = "free") +
    coord_flip()
```

-   Techno songs are high in danceability and low in acousticness

(Linear Euclidean) ordination biplot to show at all numeric features at once:

```{r}
pca <-
  track_audio_features |>
  semi_join(tracks_train) |>
  column_to_rownames("id") |>
  select(selected_audio_features) |>
  mutate(across(everything(), scale)) |>
  filter(if_any(everything(), ~ ! is.na(.x))) |>
  prcomp()

tracks_pca <-
    pca$x |>
    as_tibble(rownames = "id") |>
    left_join(track_audio_features, by = "id") |>
    left_join(track_searches, by = "id")

# get medoids
track_clusters <-
  tracks_pca |>
  group_by(term) |>
  summarise(across(c(PC1, PC2), median))

tibble() |>
  ggplot(aes(x = PC1, y = PC2, color = group)) +
  geom_text(
     data = track_clusters |> mutate(group = "term"),
     mapping = aes(label = term)
  ) +
  geom_text(
     data = pca$rotation |> as_tibble(rownames = "feature") |> mutate(group = "feature"),
     mapping = aes(label = feature)
  )
```

More detailed biplot:

```{r}
tibble() |>
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(
    data = tracks_pca,
    mapping = aes(color = term),
    alpha = 0.3
  ) +
  ggrepel::geom_label_repel(
     data = track_clusters,
     mapping = aes(label = term, color = term)
  ) +
  guides(color = FALSE) +
  ggnewscale::new_scale_color() +
  geom_segment(
    data = pca$rotation |> as_tibble(rownames = "feature"),
    mapping = aes(x = 0, y = 0, xend = max(abs(pca$x[,1])) * PC1, yend = max(abs(pca$x[,2])) * PC2),
    arrow = arrow()
  ) +
  ggrepel::geom_label_repel(
    data = pca$rotation |> as_tibble(rownames = "feature"),
    mapping = aes(label = feature, x = max(abs(pca$x[,1])) * PC1, y = max(abs(pca$x[,2])) * PC2)
  )
```

Sanity checks:

-   classical track are associated with acousticness

-   rock and techno tracks are associated with loudness

There is no clear separation between the genre clusters suggesting a complicated classification task.

## Genre prediction based on summary values

```{r}
library(tidymodels)
tar_load(model_data)

train <-
  track_audio_features |>
  filter(id %in% rownames(model_data$train_y)) |>
  left_join(track_searches, by = "id") |>
  mutate(term = term |> factor()) |>
  select(term, selected_audio_features)

test <-
  track_audio_features |>
  filter(id %in% rownames(model_data$test_y)) |>
  left_join(track_searches, by = "id") |>
  mutate(term = term |> factor()) |>
  select(term, selected_audio_features)
```

Let's start with a (linear) Support Vector Machine:

```{r}
svm_linear(mode = "classification") |>
  fit(term ~ ., data = train) |>#
  predict(test) |>
  bind_cols(test) |>
  mutate(across(c("term", ".pred_class"), ~ factor(.x, levels = test$term |> unique()))) |>
  accuracy(truth = term, estimate = .pred_class)
```

A (non-linear) random forest showed similar performance:

```{r}
rand_forest(mode = "classification") |>
  fit(term ~ ., data = train) |>
  predict(test) |>
  bind_cols(test) |>
  mutate(across(c("term", ".pred_class"), ~ factor(.x, levels = test$term |> unique()))) |>
  accuracy(truth = term, estimate = .pred_class)
```

Test accuracy was very high in general. Can it even improved using the individual pitch sequences instead of relying on just a few summary values describing the entire track?

## Genre prediction based on the pitch sequence

```{r}
summary(pca)$importance["Cumulative Proportion","PC2"]
```

Almost half of the variance can be explained by the first principal components, motivating the prediction of the terms based on the features. These features were also significantly different across the terms:

```{r}
features |>
  paste0(collapse = "+") |>
  paste0("~ term") |>
  lm(data = tracks) |>
  anova()
```

```{r}
features |>
  paste0(collapse = "+") |>
  paste0("~ term") |>
  lm(data = tracks) |>
  lm() |>
  summary()
```

Some features are highly correlated, suggesting redundancy, e.g. :

```{r}
tracks |>
  ggplot(aes(danceability, loudness)) +
    geom_point() +
    stat_smooth(method = "lm") +
    stat_cor()
```

Indeed, lots of features were significantly correlated after FDR adjustment:

```{r}
tracks |>
  select(selected_audio_features) |>
  as.matrix() |>
  Hmisc::rcorr() |>
  broom::tidy() |>
  ungroup() |>
  mutate(q.value = p.value |> p.adjust(method = "fdr")) |>
  filter(q.value < 0.05 & abs(estimate) > 0.2) |>
  arrange(-abs(estimate)) |>
  unite(col = comparision, column1, column2, sep = " vs. ") |>
  head(10) |>
  ggplot(aes(comparision, estimate)) +
    geom_col() +
    coord_flip() +
    labs(y = "Pearson correlation")
```

Music is composed of shorter and longer patterns. We can make use of the temporal property by doing convolutions on the time axis while using loudness of pitch frequencies as features.

[Spotify audio analysis](https://developer.spotify.com/community/showcase/spotify-audio-analysis/) separates the track into many segments and calculates the loudness for each of the 12 pitches (half steps) of the scale.

```{r}
track_audio_analyses$audio_analysis[[1]]$segments$pitches[1][[1]]
```

These are spectrograms of a subset of tracks representing the feature space for deep learning:

```{r}
#| fig-height: 10

track_pitches |>
  left_join(track_searches) |>
  sample_frac(0.01) |>
  select(id, term, pitches) |>
  unnest(pitches) |>
  group_by(id) |>
  mutate(segment = row_number()) |>
  pivot_longer(starts_with("V"), names_to = "pitch_name", values_to = "pitch") |>
  mutate(pitch_name = pitch_name |> str_extract("[0-9]+") |> as.numeric() |> factor()) |>
  group_by(term, segment, pitch_name) |>
  summarise(pitch = median(pitch)) |>

  ggplot(aes(segment, pitch_name)) +
      geom_tile(aes(fill = pitch)) +
      facet_wrap(~term, ncol = 1) +
      scale_fill_viridis_c() +
      scale_x_continuous(limits = c(0, 800), expand = c(0, 0)) +
      labs(x = "Time (segment)", y = "Pitch (semitone)", fill = "Median loudness")
```

```{r}
tar_load(model_archs)

model_archs$base() |> plot()
```

```{r}
model_archs$cnn1() |> plot()
```
```{r}
model_archs$cnn2() |> plot()
```

```{r}
model_archs$lstm() |> plot()
```

## Evaluate models

```{r}
tar_load(evaluations)
evaluations
```

```{r}
evaluations |>
  select(name, model) |>
  mutate(history = model |> map(~ str_glue("tmp/train_history/{.x}.csv") |> read_csv())) |>
  unnest(history) |>
  pivot_longer(c("accuracy", "val_accuracy"), names_to = "subset") |>
  mutate(subset = subset |> recode(accuracy = "train", "val_accuracy" = "validation")) |>
  ggplot(aes(epoch, value, color = subset)) +
    geom_line() +
    facet_wrap(~name, ncol = 1) +
    labs(y = "Accuracy")
```

The base and CNN1 model generalize well on the external validation samples, whereas CNN2 is affected by over-fitting.
This is maybe due to the high number of trainable parameters.
