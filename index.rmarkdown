---
title: "Predicting music genre from spotify using deep learning"
author: "Daniel Loos"
date: today
editor: visual
format:
  html:
    toc: true
---

```{r setup, echo=FALSE}

knitr::opts_knit$set(root.dir = here::here())

options(warn=-1)
```


[This project on GitHub](https://github.com/danlooo/spotify-datasci)

## Abstract

Music genres feature particular patterns that can be used to predict the genre of a track by analyzing its pitch sequences. The Spotify API provides features for entire tracks, e.g. its loudness or acousticness scores, as well as the sequence of the pitches. About half the the variance of totaling 3600 tracks across techno, rock, jazz and classicsal generes could be explained by these summary features alone. However, there was no strong separation between the genre clusters. Deep learning was then applied to find patterns of the detailed pitch sequence. However, only 30% of the test tracks could be classified correctly using CNN or LSTM architectures.

## ETL pipeline


```{r}
source("_targets.R")
tar_load(c("terms", "track_audio_features", "selected_audio_features", "audio_analyses", "track_train_test_split", "track_searches", "track_pitches", "valid_tracks", "track_audio_analyses"))

tar_visnetwork()
```


## Data overview

Spotify was queried by the following terms. Up to 50 tracks per term were retrieved.


```{r}
terms
```


Total number of tracks:


```{r}
nrow(track_searches)
```


Tracks per term:


```{r}
track_searches |> count(term)
```


Features per track:


```{r}
tracks <-
  track_audio_features |>
  left_join(track_searches, by = "id") |>
  filter(id %in% valid_tracks) |>
  mutate(term = factor(term))

colnames(tracks)
```


Number of tracks after sanity checks:


```{r}
nrow(tracks)
```


Split train/test data to not bias any analysis. Stratify by term to ensure a balanced data set.


```{r}
tar_load(track_train_test_split)

tracks_train <- tracks |> inner_join(track_train_test_split) |> filter(is_train)
tracks_test <- tracks |> anti_join(tracks_train)
```


## Track features per term


```{r}
features <- c("danceability", "acousticness")

tracks_train |>
  select(term, features) |>
  mutate(across(features, scale)) |>
  pivot_longer(features) |>
  ggplot(aes(term, value)) +
    geom_quasirandom() +
    geom_boxplot(outlier.size = NULL, width = 0.5) +
    facet_wrap(~ name, scales = "free") +
    coord_flip()
```


-   Techno songs are high in danceability and low in acousticness

(Linear Euclidean) ordination biplot to show at all numeric features at once:


```{r}
pca <-
  track_audio_features |>
  semi_join(tracks_train) |>
  column_to_rownames("id") |>
  select(selected_audio_features) |>
  mutate(across(everything(), scale)) |>
  filter(if_any(everything(), ~ ! is.na(.x))) |>
  prcomp()

tracks_pca <-
    pca$x |>
    as_tibble(rownames = "id") |>
    left_join(track_audio_features, by = "id") |>
    left_join(track_searches, by = "id")

# get medoids
track_clusters <-
  tracks_pca |>
  group_by(term) |>
  summarise(across(c(PC1, PC2), median))

tibble() |>
  ggplot(aes(x = PC1, y = PC2, color = group)) +
  geom_text(
     data = track_clusters |> mutate(group = "term"),
     mapping = aes(label = term)
  ) +
  geom_text(
     data = pca$rotation |> as_tibble(rownames = "feature") |> mutate(group = "feature"),
     mapping = aes(label = feature)
  )
```


More detailed biplot:


```{r}
tibble() |>
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(
    data = tracks_pca,
    mapping = aes(color = term),
    alpha = 0.3
  ) +
  ggrepel::geom_label_repel(
     data = track_clusters,
     mapping = aes(label = term, color = term)
  ) +
  guides(color = FALSE) +
  ggnewscale::new_scale_color() +
  geom_segment(
    data = pca$rotation |> as_tibble(rownames = "feature"),
    mapping = aes(x = 0, y = 0, xend = max(abs(pca$x[,1])) * PC1, yend = max(abs(pca$x[,2])) * PC2),
    arrow = arrow()
  ) +
  ggrepel::geom_label_repel(
    data = pca$rotation |> as_tibble(rownames = "feature"),
    mapping = aes(label = feature, x = max(abs(pca$x[,1])) * PC1, y = max(abs(pca$x[,2])) * PC2)
  )
```


Sanity checks:

-   classical track are associated with acousticness

-   rock and techno tracks are associated with loudness

There is no clear separation between the genre clusters suggesting a complicated classification task.

## Predicting the term based on the musical features


```{r}
summary(pca)$importance["Cumulative Proportion","PC2"]
```


Almost half of the variance can be explained by the first principal components, motivating the prediction of the terms based on the features. These features were also significantly different across the terms:


```{r}
features |>
  paste0(collapse = "+") |>
  paste0("~ term") |>
  lm(data = tracks) |>
  anova()
```

```{r}
features |>
  paste0(collapse = "+") |>
  paste0("~ term") |>
  lm(data = tracks) |>
  lm() |>
  summary()
```


44% of the variance could be explained in a linear way. Can we do better using the pitches over time as features?

## Modeling sequences of pitches

Some features are highly correlated, suggesting redundancy, e.g. :


```{r}
tracks |>
  ggplot(aes(danceability, loudness)) +
    geom_point() +
    stat_smooth(method = "lm") +
    stat_cor()
```


Indeed, lots of features were significantly correlated after FDR adjustment:


```{r}
tracks |>
  select(selected_audio_features) |>
  as.matrix() |>
  Hmisc::rcorr() |>
  broom::tidy() |>
  ungroup() |>
  mutate(q.value = p.value |> p.adjust(method = "fdr")) |>
  filter(q.value < 0.05 & abs(estimate) > 0.2) |>
  arrange(-abs(estimate)) |>
  unite(col = comparision, column1, column2, sep = " vs. ") |>
  head(10) |>
  ggplot(aes(comparision, estimate)) +
    geom_col() +
    coord_flip() +
    labs(y = "Pearson correlation")
```


Music is composed of shorter and longer patterns. We can make use of the temporal property by doing convolutions on the time axis while using loudness of pitch frequencies as features.

[Spotify audio analysis](https://developer.spotify.com/community/showcase/spotify-audio-analysis/) separates the track into many segments and calculates the loudness for each of the 12 pitches (half steps) of the scale.


```{r}
track_audio_analyses$audio_analysis[[1]]$segments$pitches[1][[1]]
```


First, only use intensity scores of note c to get a 2D data frame:


```{r}
c_pitches <-
  track_pitches |>
  filter(id %in% valid_tracks) |>
  unnest(pitches) |>
  group_by(id) |>
  mutate(segment = row_number()) |>
  select(id, segment, V1) |>
  pivot_wider(names_from = segment, values_from = V1)

c_pitches |>
  left_join(track_train_test_split) |>
  filter(is_train) |>
  select(-is_train) |>
  pivot_longer(-id, names_to = "segment") |>
  left_join(track_searches) |>
  type_convert() |>
  ggplot(aes(segment, value)) +
    geom_density_2d_filled() +
    facet_wrap(~term, ncol = 1)
```


Classical songs tend to be more consistent across the segment time samples.

